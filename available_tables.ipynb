{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of companies\n",
    "# 0C000008MR, 0C00000AXM, 0C00000BUG, 0C000007R3, 0C000009JZ\n",
    "\n",
    "# SHARECLASS\n",
    "# 0P0000A5G3, 0P00006WAE, 0P000002H5, 0P00007O7R, 0P000003PC\n",
    "\n",
    "# Need to install 'pyathena' in current docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_shareclassid = ['0P0000A5G3', '0P00007O7R', '0P000003PC']\n",
    "user_start_date = '2002-05-06'\n",
    "user_end_date = '2007-05-07'\n",
    "\n",
    "dates = (user_start_date, user_end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyathena\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.master('local').\\\n",
    "        appName('Raw_data_analysis').\\\n",
    "        config('spark.driver.memory','20G').\\\n",
    "        config(\"spark.executor.memory\",\"5gb\").\\\n",
    "        config(\"spark.cores.max\",\"6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morningstar Domain Username (root): schava3\n",
      "Federating through ADFS Prod using script version 2.0.5\n",
      "Username: schava3@morningstar.com\n",
      "Region: us-east-1\n",
      "Profile: default\n",
      "Keyring: OS error: No module named 'keyring'\n",
      "········\n",
      "Please choose the role you would like to assume:\n",
      "0) quant-non-prod-operator (809853243180)\n",
      "1) quant-prod-readonly (763485997594)\n",
      "2) apg-non-prod-operator (957772357715)\n",
      "3) bit-non-prod-dev-general (309603612345)\n",
      "4) bit-prod-production (156286860437)\n",
      "5) bit-prod-uat (156286860437)\n",
      "Role Integer eg. 0,1,2,all: 0\n",
      "----------------------------------------------------------------\n",
      "Keys saved under profile: [ default ] \n",
      "Keys File: /root/.aws/credentials\n",
      "Sessions Expiration: 2020-09-28 15:47:33+00:00. (12 hour)\n",
      "After this time, you may safely rerun this script to refresh your access key pair.\n",
      "To use this credential, create an AWS sessionin the CLI using the profile name default\n",
      "Examples:\n",
      "  Boto3: session = boto3.session.Session(profile_name='default')\n",
      "  AWScli: $ aws --profile default s3 ls s3://\n",
      "----------------------------------------------------------------\n",
      "SAML Token info: \n",
      "profile = default\n",
      "role_arn = arn:aws:iam::809853243180:role/mstar-quant-non-prod-operator\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Morningstar/amazon_adfs/aws-sts-forms-v2.py:46: DeprecationWarning: This method will be removed in future versions.  Use 'parser.read_file()' instead.\n",
      "  config.readfp(open(filename))\n"
     ]
    }
   ],
   "source": [
    "%run /Morningstar/amazon_adfs/aws-sts-forms-v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Settings for connecting to athena database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "db_name = 'rm_2020_08_18_prod_glb_eq_usd_fof'\n",
    "\n",
    "conn = pyathena.connect(s3_staging_dir='s3://risk-model-2.0-temp', region_name='us-east-1')\n",
    "\n",
    "timeindex_query = pd.read_sql(f\"\"\"select timeindex from {db_name}.riskmodel__timeindex where \\\n",
    "            modeldate between date '{user_start_date}' and date '{user_end_date}' order by timeindex\"\"\", conn)\n",
    "\n",
    "ti = [timeindex_query['timeindex'].iloc[0], timeindex_query['timeindex'].iloc[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r_securityid_query=pd.read_sql(\"select * from {}.riskmodel__security_map where shareclassid in \\\n",
    "                               ({})\".format(db_name, \",\".join(\"'{0}'\".format(i) for i in user_shareclassid)), conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_r_securityid = list(r_securityid_query['r_securityid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(\"/Morningstar/audit_trail/list_of_tables/*\")\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final filters of user input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"/Morningstar/risk-model-data/tmp/fakemodelid/\", topdown=False):\n",
    "    for name in dirs:\n",
    "        if name not in ['buildpremia', 'etl']:\n",
    "            raw_spark_df = spark.read.parquet(os.path.join(root, name))\n",
    "            \n",
    "            if 'SHARECLASSID' in raw_spark_df.columns:\n",
    "\n",
    "                filter_company = raw_spark_df.filter((F.col('SHARECLASSID').isin(user_shareclassid)))\n",
    "                \n",
    "            elif 'R_SECURITYID' in raw_spark_df.columns:\n",
    "                \n",
    "                filter_company = raw_spark_df.filter(F.col('R_SECURITYID').isin(user_r_securityid))\n",
    "                \n",
    "            if 'EFFECTIVEDATE' in filter_company.columns:\n",
    "\n",
    "                filter_dates = filter_company.where(F.col('EFFECTIVEDATE').between(*dates))\n",
    "\n",
    "                final_data = filter_dates.sort(F.col('EFFECTIVEDATE'), ascending=False)\n",
    "\n",
    "            elif 'TIMEINDEX' in filter_company.columns:\n",
    "                \n",
    "                filter_dates = filter_company.filter((F.col('TIMEINDEX') >= int(ti[0])) & (F.col('TIMEINDEX') <= int(ti[1])))\n",
    "\n",
    "                final_data = filter_dates.sort(F.col('TIMEINDEX'), ascending=False)\n",
    "            \n",
    "            final_data.toPandas().to_csv(\"/Morningstar/audit_trail/list_of_tables/{0}.csv\".format(name), index=False)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
